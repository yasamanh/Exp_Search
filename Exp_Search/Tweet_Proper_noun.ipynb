{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import CMUTweetTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ynh3/clustering/algo_test/lib/python2.7/site-packages/gensim-0.10.2-py2.7-macosx-10.9-intel.egg/gensim/__init__.py:10: UserWarning: Module gensim was already imported from /Users/ynh3/clustering/algo_test/lib/python2.7/site-packages/gensim-0.10.2-py2.7-macosx-10.9-intel.egg/gensim/__init__.pyc, but /Users/ynh3/clustering/algo_test/bin is being added to sys.path\n",
      "  __version__ = __import__('pkg_resources').get_distribution('gensim').version\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "from nltk import Text\n",
    "from nltk.tokenize import line_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import *\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with generating tweets_table\n"
     ]
    }
   ],
   "source": [
    "f1 = 'statuses.log.2015-04-08-00'\n",
    "fpath = '/Users/ynh3/Data/Tweets_2015/'\n",
    "#fpath = '/Users/ynh3/Data/Tweets_2015_rt/'\n",
    "\n",
    "###This reads values from the asci file and saves the actual tweets in tweets_table\n",
    "file = open(fpath + f1)\n",
    "i = 0\n",
    "tweets_table={}\n",
    "for row in file:\n",
    "    tweet = row.strip()\n",
    "    tweet_text = tweet.split(' ',0)[0]\n",
    "\n",
    "    list_tweet=[]\n",
    "    list_tweet.append(tweet_text)\n",
    "    i=i+1\n",
    "    tweets_table[i]=list_tweet\n",
    "\n",
    "values = tweets_table.viewvalues()\n",
    "file.close()\n",
    "temp = list(values)\n",
    "tweets = [x[0] for x in temp]\n",
    "print \"Done with generating tweets_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36419"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_url= [(i,t) for i,t in enumerate(tweets) if \"http://\" in t or \"https://\" in t]\n",
    "len(tweets_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'Pray for us sinners #mother https://t.co/iKfL8GPvQ5')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @16KVLO: My new sounds: Soul Musiq(Prod. Yung Prodigy &amp; Bree Honcho) https://t.co/w4ueXKkL7H on #SoundCloud'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46738391393032835"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage\n",
    "float(len(tweets_url))/float(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_url_value = [tw[1].replace('\\r', ' ') for tw in tweets_url]\n",
    "#tweets_cleaned = [tw.replace('\\r', ' ') for tw in s]\n",
    "len(tweets_url_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @AkanshaGautam_: Little lady by Camelia Mot @SusanGilbert @Cynthiapoet @drkent @_Akanshagautam http://t.co/8Cm4xMaPKd'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_url_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list = CMUTweetTagger.runtagger_parse(tweets_url_value)\n",
    "#tweets_entity = [y for y in tagged_list for e in y if e[1]=='N' or e[1]=='^']\n",
    "len(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_entity=[]\n",
    "#keep a list of actual tweets\n",
    "tweets_list=[]\n",
    "for y in tagged_list:\n",
    "    for e in y:\n",
    "        if e[1]=='^':\n",
    "            tweets_entity.append(e[0])\n",
    "            i = tagged_list.index(y)\n",
    "            tweets_list.append(tweets_url[i][1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6026"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#badoo']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_entity[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As one of the hottest people on #badoo, I can show off about it all week on http://t.co/7GWsLXJZIF http://t.co/SZ0UEgiO2w',\n",
       " 'Nexxen NE8801M 2T/SIL/BLK http://t.co/lDHzuxzgoB #Nexxen',\n",
       " 'Wcw goes out to my fire baby http://t.co/BmFeek6AeD',\n",
       " 'I posted a new photo to Facebook http://t.co/mqZnUx0Sj4',\n",
       " 'A decent afternoon in Naches for a ZHS track meet... http://t.co/digwlNEw1Q']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6027"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6027"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10805"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16549054065185756"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage\n",
    "float(len(tweets_list))/float(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5577973160573808"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage\n",
    "float(len(tweets_list))/float(len(tweets_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @AkanshaGautam_: Little lady by Camelia Mot @SusanGilbert @Cynthiapoet @drkent @_Akanshagautam http://t.co/8Cm4xMaPKd', 'RT @ThePlayersWorld: Cinque terre, Italy villas and scenery http://t.co/CmM419fDuU', 'RT @16KVLO: My new sounds: Soul Musiq(Prod. Yung Prodigy &amp; Bree Honcho) https://t.co/w4ueXKkL7H on #SoundCloud', 'RT @cuckoldforcedbi: http://t.co/wMCFtFoxPB Dana Vespoli Gets Naughty! http://t.co/bPpzUbObty', \"RT @Mandisa09150378: I hate you Perrie you made my life a mess because of you zayn can't be in 1D I'm going to unfollow you http://t.co/ORK\", 'RT @Ashton5SOS: Played Cowboys for a day out in the desert  http://t.co/FaRGd2fxt3', 'RT @75blinkhes: NIALL AND SPORTS IS SO CUTE EVEN THOUGH HIS UPDATES BORE US IT MAKES HIM HAPPY AND I LOVE HIM http://t.co/1ym6pq3WXg', 'RT @AmaZeGoD: Me and my duo @CRUClFIED puttin im that work tonight. Even though we lost it was a really GG. @MLGTechh @RiftLegacy http://t.', 'RT @GovChristie: What does a Girl Scout, the  NYTimes and the current FBI Director have in common? They are all in this video--&gt; https://t.', 'RT @NBCNewYork: 100-year-old New Jersey man kills sleeping 88-year-old wife with ax, slits own wrists: officials http://t.co/zF8rPzZpob']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3690"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_enumerate = [e for e in set(tweets_entity)]\n",
    "len(entity_enumerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPx',\n",
       " 'Savage',\n",
       " 'En',\n",
       " '#3D',\n",
       " 'Kensington',\n",
       " 'EliteField',\n",
       " 'Thalassaemia',\n",
       " '#EURUSD',\n",
       " 'Olympics',\n",
       " '#supernatural']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_enumerate[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = set([line.replace(\"\\n\",\"\") for line in open('/Users/ynh3/Data/stopword-list.txt')])\n",
    "pun = [',', '-','.', ':', '(', ')', '--', ';', '...', 'may', 'must', 'us', 'via','a','the', 'rt', 'gg', 'gt', 'lt', 'la', 'de', 'te', 'lol', 'follow', 'followers', 'unfollow', 'unfollowers', 'unfollower', 'follower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with tokenizations. tweets count:  6010\n"
     ]
    }
   ],
   "source": [
    "    ###TOKENIZING\n",
    "    tknzr = TweetTokenizer()   \n",
    "    tweet_count = 0\n",
    "    tweet_tokens=[]\n",
    "    new_tweet_list=[]\n",
    "    #regex and then line_tokenize, so each line is one tweet (one problem: two line tweets will become two tweets.)\n",
    "    i = 0\n",
    "    \n",
    "    for line in tweets_list:\n",
    "  \n",
    "        new_l = tknzr.tokenize(line)\n",
    "        lower_l = [e.lower() for e in new_l]\n",
    "        new_ll = [e for e in lower_l if e not in stoplist and e not in pun and e.isalpha() and len(e) > 2]\n",
    "\n",
    "        if len(new_ll) > 0:\n",
    "            tweet_tokens.append(new_ll)\n",
    "            tweet_count += 1\n",
    "            new_tweet_list.append(line)\n",
    "            #new_list=[]\n",
    "            #new_list.append(new_ll)\n",
    "            #tweets_list.append(new_list)'''\n",
    "        i += 1\n",
    "    print \"Done with tokenizations. tweets count: \" , tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###GENSIM MODELING AND MAKING DICTIONARY\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in new_tweet_list)\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "#once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq < 5]\n",
    "dictionary.filter_tokens(stop_ids) \n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   ###LSI TOPIC MODELING BASED ON OPT\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in tweet_tokens:\n",
    "            # assume there's one document per line\n",
    "            yield dictionary.doc2bow(line)\n",
    "\n",
    "gensim_corpus = MyCorpus()\n",
    "\n",
    "tfidf = models.TfidfModel(gensim_corpus)\n",
    "corpus_tfidf = tfidf[gensim_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI model takes 2.751060009 seconds\n",
      "lsi is taking 0.60023021698 seconds\n"
     ]
    }
   ],
   "source": [
    "    n_cluster = 50\n",
    "    # LSI computing\n",
    "    t1=time.time()\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=n_cluster) # initialize an LSI transformation\n",
    "    corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "    t2 = time.time()\n",
    "    print 'LSI model takes {} seconds'.format(t2 - t1)\n",
    "\n",
    "    # Creating LSI Label for each document\n",
    "    lsi_topic_labels=[]\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    t1=time.time()\n",
    "    #topicmax is numpy.ndarray\n",
    "    for i,doc in enumerate(lsi[corpus_tfidf]):\n",
    "        a = np.array(doc)\n",
    "        absA= abs(a)\n",
    "        if (len(a)==0):\n",
    "            cluster_id=0\n",
    "            prob_value=0.0\n",
    "            j=j+1\n",
    "        else:\n",
    "            try:\n",
    "                topicmax = absA.argmax(axis=0)\n",
    "            except ValueError:\n",
    "                print \"ValueError at\"\n",
    "                print i\n",
    "                #print a\n",
    "            cluster_id = a[topicmax.item(1)][0]\n",
    "            prob_value = abs(a[topicmax.item(1)][1])\n",
    "\n",
    "        triple = i, cluster_id, prob_value\n",
    "        lsi_topic_labels.append(triple)\n",
    "        i=i+1\n",
    "    t2=time.time()\n",
    "    print 'lsi is taking {} seconds'.format(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of lsi_topic_labels is 6010\n",
      "length of cluster_list is 6010\n",
      "length of counter_sort is 50\n",
      "n_cluster is 50\n",
      "cid_sort is 50\n"
     ]
    }
   ],
   "source": [
    "    topics = []\n",
    "    for i in range(0, n_cluster):\n",
    "        l = lsi.print_topic(i,topn=5)\n",
    "        prob_list = [(abs(float(a[:5])),a[6:])  for a in (l.split('+ ')) if abs(float(a[:5])) > 0.3]\n",
    "        sorted(prob_list)\n",
    "        topics.append(prob_list)\n",
    "\n",
    "\n",
    "    ###MAKING READY FOR\n",
    "    cluster_topic_list = sorted(lsi_topic_labels,key=itemgetter(1,2), reverse=True)\n",
    "\n",
    "    cluster_list = sorted(lsi_topic_labels,key=lambda x: x[1])\n",
    "    counter = Counter(b for a,b,c in cluster_list)\n",
    "    counter_sort = counter.most_common(n_cluster)\n",
    "    cid_sort = [e[0] for e in counter_sort]\n",
    "\n",
    "    print 'length of lsi_topic_labels is {}'.format(len(lsi_topic_labels))\n",
    "    print 'length of cluster_list is {}'.format(len(cluster_list))\n",
    "    print 'length of counter_sort is {}'.format(len(counter_sort))\n",
    "    print 'n_cluster is {}'.format(n_cluster)\n",
    "    print 'cid_sort is {}'.format(len(cid_sort))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.41, u'*\"twitter\" '),\n",
       "  (0.37, u'*\"click\" '),\n",
       "  (0.37, u'*\"want\" '),\n",
       "  (0.36, u'*\"meet\" '),\n",
       "  (0.36, u'*\"girls\"')],\n",
       " [(0.59, u'*\"paul\" '), (0.58, u'*\"walker\" '), (0.53, u'*\"rip\" ')],\n",
       " [(0.45, u'*\"spots\" '),\n",
       "  (0.45, u'*\"surf\" '),\n",
       "  (0.45, u'*\"bali\" '),\n",
       "  (0.45, u'*\"beginners\" '),\n",
       "  (0.42, u'*\"best\"')],\n",
       " [(0.71, u'*\"outfit\" '), (0.69, u'*\"point\" ')],\n",
       " [(0.449, u'\"steroid\" '),\n",
       "  (0.449, u'\"gaining\" '),\n",
       "  (0.449, u'\"edge\" '),\n",
       "  (0.446, u'\"athletes\" '),\n",
       "  (0.441, u'\"legal\"')],\n",
       " [(0.78, u'*\"like\" '), (0.33, u'*\"fav\" ')],\n",
       " [(0.498, u'\"hair\" ')],\n",
       " [(0.53, u'*\"instagram\" '), (0.34, u'*\"likes\" ')],\n",
       " [(0.576, u'\"mom\" '), (0.576, u'\"bitmoji\" '), (0.576, u'\"discovers\" ')],\n",
       " [(0.41, u'*\"cutie\" '),\n",
       "  (0.41, u'*\"ceramic\" '),\n",
       "  (0.41, u'*\"africanurbanart\" '),\n",
       "  (0.41, u'*\"giraffe\" '),\n",
       "  (0.4, u'*\"african\"')]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 49.0, 0.0154188733599947)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_topic_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 0.0, 0.063730617292470565)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets is 4561\n"
     ]
    }
   ],
   "source": [
    "###WRITING JSON FOR HASH\n",
    "def jdefault(o):\n",
    "    return o.__dict__\n",
    "\n",
    "output_file = open(fpath + 'tweet-topic.txt', 'w')\n",
    "#json_file = open(jpath + f1 + '-'+ opt + 'wo-cw-ch.json', 'w')\n",
    "#topic_lsi_file = open(cpath + f1 + opt + '.cluster', 'w')\n",
    "data_list=[]\n",
    "data = {}\n",
    "count=0\n",
    "i=0\n",
    "\n",
    "for i in range(0, len(cid_sort)):\n",
    "    sublist = [e for e in cluster_topic_list if e[1] == cid_sort[i]]\n",
    "    \n",
    "    t_index = int(cid_sort[i])\n",
    "    topic = topics[t_index]\n",
    "    string = [x[1] for x in topic]\n",
    "    string = ' '.join(string)\n",
    "    topic_str = string.replace(\"\\\"\",\"\")\n",
    "    output_file.write(topic_str)\n",
    "    output_file.write('-----------------------\\n')\n",
    "    \n",
    "    for j in range(0,len(sublist)):\n",
    "        data = {}\n",
    "        index = sublist[j][0]\n",
    "        if (index >= len(new_tweet_list)):\n",
    "            print index, i, j\n",
    "            break\n",
    "        tweet_str = new_tweet_list[index]\n",
    "        \n",
    "        data['id']=int(count+1)\n",
    "        data['topic']=topic_str\n",
    "        data['tweet']=tweet_str\n",
    "        data_list.append(data)\n",
    "        #k = k+1\n",
    "        count=count+1\n",
    "        output_file.write(tweet_str+'\\n')\n",
    "\n",
    "        count_str = str(count)\n",
    "\n",
    "#json_file.write('{\"total\":')\n",
    "#json_file.write(count_str)\n",
    "#json_file.write(',')\n",
    "#json_file.write('\"tweets\": [')\n",
    "\n",
    "'''for p in range(0,len(data_list)):\n",
    "    record = data_list[p]\n",
    "    json_data = json.dumps(record, default=jdefault)\n",
    "    json_file.writelines(json_data)\n",
    "    if (p < len(data_list) - 1 ):\n",
    "        json_file.write(',')\n",
    "\n",
    "json_file.write(']}')\n",
    "json_file.close()'''\n",
    "print(\"Total number of tweets is {}\".format(count))\n",
    "output_file.close()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 2,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 3,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 4,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 5,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 6,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 7,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 8,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: PM Shri @narendramodi launches the logo of #MUDRA http://t.co/X9QVLHr5iQ'},\n",
       " {'id': 9,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: MUDRA Launch by PM Shri @narendramodi today at 9:45 am, Vigyan Bhawan, New Delhi. http://t.co/Ly86HLGzD0'},\n",
       " {'id': 10,\n",
       "  'topic': u'shri ',\n",
       "  'tweet': 'RT @MIB_India: MUDRA Launch by PM Shri @narendramodi today at 9:45 am, Vigyan Bhawan, New Delhi. http://t.co/Ly86HLGzD0'}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-374f5153b298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtopic_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtopic_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "topic_list = [x[1] for x in y for y in topics]\n",
    "topic_set = set(topic_list)\n",
    "topic_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_list = [x[i][1] for x in topics for i in range(0,len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'*\"twitter\" ',\n",
       " u'*\"click\" ',\n",
       " u'*\"want\" ',\n",
       " u'*\"meet\" ',\n",
       " u'*\"girls\"',\n",
       " u'*\"paul\" ',\n",
       " u'*\"walker\" ',\n",
       " u'*\"rip\" ',\n",
       " u'*\"spots\" ',\n",
       " u'*\"surf\" ']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "t=[]\n",
    "for x in topics:\n",
    "    m = [x[i][1] for i in range(0,len(x))]\n",
    "    if (len(m)>0):\n",
    "        t.append(m)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'*\"weather\" ', u'*\"update\" '],\n",
       " [u'*\"new\" ', u'*\"week\" ', u'*\"twitter\" ', u'*\"posted\" ', u'*\"facebook\"'],\n",
       " [u'\"posted\" ', u'\"facebook\" ', u'\"photo\" ', u'*\"twitter\" ', u'*\"week\"'],\n",
       " [u'\"apple\" ', u'\"watch\" '],\n",
       " [u'\"vote\" ', u'\"trevor\" '],\n",
       " [u'*\"april\" '],\n",
       " [u'\"collected\" ', u'\"coins\" ', u'\"gold\" '],\n",
       " [u'\"read\" ', u'\"full\" ', u'\"ebay\" ', u'*\"apple\" '],\n",
       " [u'*\"here\" ', u'*\"video\" '],\n",
       " [u'*\"harvested\" ', u'*\"food\" '],\n",
       " [u'*\"see\" ', u'*\"who\" '],\n",
       " [u'\"video\" ', u'*\"here\" ', u'\"liked\" '],\n",
       " [u'\"now\" ', u'\"online\" '],\n",
       " [u'*\"love\" ', u'*\"now\" '],\n",
       " [u'\"love\" ', u'*\"black\" '],\n",
       " [u'*\"love\" ', u'\"now\" ', u'*\"black\" '],\n",
       " [u'\"earning\" '],\n",
       " [u'*\"live\" ', u'*\"like\" '],\n",
       " [u'\"free\" ', u'*\"live\" '],\n",
       " [u'*\"live\" ', u'\"like\" '],\n",
       " [u'*\"just\" ', u'\"free\" '],\n",
       " [u'\"camgirl\" ', u'\"twitter\" '],\n",
       " [u'*\"people\" ', u'*\"followed\" ', u'\"twitter\" '],\n",
       " [u'*\"people\" '],\n",
       " [u'\"camgirl\" '],\n",
       " [u'\"click\" '],\n",
       " [u'*\"set\" '],\n",
       " [u'\"will\" '],\n",
       " [u'\"can\" ', u'\"check\" '],\n",
       " [u'\"officer\" '],\n",
       " [u'\"apr\" ', u'\"leader\" ', u'\"wed\" ', u'\"burn\" '],\n",
       " [u'*\"trevor\" ', u'\"pls\" '],\n",
       " [u'\"can\" '],\n",
       " [u'\"peek\" ', u'\"sneak\" ', u'\"scenes\" ', u'\"checkout\" '],\n",
       " [u'\"check\" '],\n",
       " [u'\"check\" '],\n",
       " [u'*\"know\" '],\n",
       " [u'\"happy\" ', u'\"birthday\" '],\n",
       " [u'\"day\" '],\n",
       " [u'*\"best\" ', u'\"photos\" '],\n",
       " [u'*\"happy\" ', u'\"know\" '],\n",
       " [u'*\"null\" '],\n",
       " [u'\"time\" ']]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
